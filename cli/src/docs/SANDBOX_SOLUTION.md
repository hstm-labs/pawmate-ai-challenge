# Sandbox Solution for Loop-Until-Green Workflow

## Problem Statement

The benchmark prompt requires the AI tool to autonomously execute the complete loop-until-green workflow:
1. Build (npm install)
2. Seed data
3. Start API server
4. Run tests
5. Fix failures
6. Repeat tests until 100% pass

However, the default sandbox environment blocks network access, preventing `npm install` from downloading dependencies.

## Solution for This Run

This run was completed successfully using a two-phase approach:

### Phase 1: Manual Build (Operator)
```bash
cd backend
npm install  # Executed by operator with network permissions
```

Result: 108 packages installed in 18 seconds

### Phase 2: Autonomous Loop-Until-Green (AI Tool)
Once dependencies were installed, the AI tool successfully executed:
- Seed data loading
- API server start
- Test execution (3 iterations)
- Issue identification and fixes
- Retesting until 100% pass rate

## Recommendations for Future Runs

### Option 1: Pre-install Dependencies (Recommended for consistent benchmarking)

Add a pre-run setup step to the benchmark workflow:

```bash
# Before starting the AI tool
cd /path/to/workspace/backend
npm install
```

This ensures:
- Consistent package versions across runs
- No network variability during benchmarking
- AI tool can focus on code generation and testing
- More accurate timing measurements (excludes npm install variability)

### Option 2: Request Network Permissions (For fully autonomous execution)

Update the prompt to instruct the AI tool to request network permissions for the build step:

```javascript
// In the loop-until-green workflow
await run_terminal_cmd({
  command: 'cd backend && npm install',
  is_background: false,
  required_permissions: ['network']  // Request network access
});
```

### Option 3: Use Local Cache (Hybrid approach)

The `.npmrc` file already includes `prefer-offline=true` and `cache=.npm-cache`. Pre-populate the cache:

```bash
# One-time setup
cd backend
npm install --cache .npm-cache --prefer-offline
```

Then subsequent runs can use the cached packages with minimal network access.

## Impact on Benchmarking

### Current Approach (Manual build + AI loop-until-green)
- **Pros**: Network issues don't affect test timing; consistent dependencies
- **Cons**: Requires operator intervention; breaks full autonomy
- **Timing Impact**: Build time excluded from AI timestamps (but recorded separately)

### Recommended Approach (Pre-installed dependencies)
- **Pros**: Fully autonomous; consistent benchmarking; no network variability
- **Cons**: Requires setup step before each run
- **Timing Impact**: None; AI starts from code generation

## Verification

This run demonstrates successful completion with the manual build approach:

- ✅ All code generated by AI
- ✅ Dependencies installed (manual)
- ✅ Loop-until-green workflow executed autonomously
- ✅ 3 test iterations completed
- ✅ 100% pass rate achieved
- ✅ All timestamps recorded
- ✅ Total time from generation_started to all_tests_pass: ~12 minutes

## Conclusion

The sandbox restriction is solvable. For future runs, pre-installing dependencies before initiating the AI tool provides the best balance of autonomy, consistency, and accurate benchmarking.

## Prompt Update Recommendation

Add to section 3.0 (Required Tech Stack):

```markdown
**Dependency Installation:**
For benchmark consistency, dependencies SHOULD be pre-installed before starting the AI tool:
- Run `npm install` in the backend directory with network access
- This ensures consistent package versions and eliminates network variability
- AI tool timestamps begin at code generation, not dependency installation
```

Alternatively, update section 5.1 (Build Loop) to explicitly request network permissions:

```markdown
#### 5.1 Build Loop
1. Execute `npm install` using `run_terminal_cmd` with `required_permissions: ['network']`
2. If npm install fails, check `.npmrc` configuration and retry
3. Repeat until build succeeds
4. Record timestamp for `build_clean`
```

